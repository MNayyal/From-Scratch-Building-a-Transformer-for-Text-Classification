# -*- coding: utf-8 -*-
"""From Scratch - Transformers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/193Ih47P8FJELM20lKsZgs88mukVztqmA
"""

! pip install datasets

# -*- coding: utf-8 -*-
"""Transformers_from_scratch.ipynb

Implementation of Transformer model from scratch for text classification.
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

import numpy as np
from datetime import datetime
from transformers import AutoTokenizer, DataCollatorWithPadding
from datasets import load_dataset

class MultiHeadAttention(nn.Module):
    def __init__(self, d_k, d_model, n_heads):
        super().__init__()
        self.d_k = d_k
        self.n_heads = n_heads

        self.key = nn.Linear(d_model, d_k * n_heads)
        self.query = nn.Linear(d_model, d_k * n_heads)
        self.value = nn.Linear(d_model, d_k * n_heads)
        self.fc = nn.Linear(d_k * n_heads, d_model)

    def forward(self, q, k, v, mask=None):
        N = q.shape[0]  # Batch size
        T = q.shape[1]  # Sequence length

        q = self.query(q).view(N, T, self.n_heads, self.d_k).transpose(1, 2)
        k = self.key(k).view(N, T, self.n_heads, self.d_k).transpose(1, 2)
        v = self.value(v).view(N, T, self.n_heads, self.d_k).transpose(1, 2)

        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_k)

        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask[:, None, None, :] == 0, float('-inf'))

        attn_weights = F.softmax(attn_scores, dim=-1)
        A = attn_weights @ v

        A = A.transpose(1, 2).contiguous().view(N, T, self.d_k * self.n_heads)
        return self.fc(A)

class TransformerBlock(nn.Module):
    def __init__(self, d_k, d_model, n_heads, dropout_prob=0.1):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        self.mha = MultiHeadAttention(d_k, d_model, n_heads)
        self.ann = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Linear(d_model * 4, d_model),
            nn.Dropout(dropout_prob),
        )
        self.dropout = nn.Dropout(p=dropout_prob)

    def forward(self, x, mask=None):
        x = self.ln1(x + self.mha(x, x, x, mask))
        x = self.ln2(x + self.ann(x))
        x = self.dropout(x)
        return x

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=2048, dropout_prob=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout_prob)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(1, max_len, d_model)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class Encoder(nn.Module):
    def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, n_classes, dropout_prob):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)

        transformer_blocks = [
            TransformerBlock(d_k, d_model, n_heads, dropout_prob)
            for _ in range(n_layers)
        ]
        self.transformer_blocks = nn.Sequential(*transformer_blocks)

        self.ln = nn.LayerNorm(d_model)
        self.fc = nn.Linear(d_model, n_classes)

    def forward(self, x, mask=None):
        x = self.embedding(x)
        x = self.pos_encoding(x)

        for block in self.transformer_blocks:
            x = block(x, mask)

        # Use first token for classification (like BERT's [CLS] token)
        x = x[:, 0, :]
        x = self.ln(x)
        x = self.fc(x)
        return x

def train(model, criterion, optimizer, train_loader, valid_loader, epochs):
    train_losses = np.zeros(epochs)
    test_losses = np.zeros(epochs)

    for it in range(epochs):
        model.train()
        t0 = datetime.now()
        train_loss = 0
        n_train = 0

        for batch in train_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            optimizer.zero_grad()

            outputs = model(batch['input_ids'], batch['attention_mask'])
            loss = criterion(outputs, batch['labels'])

            loss.backward()
            optimizer.step()

            train_loss += loss.item() * batch['input_ids'].size(0)
            n_train += batch['input_ids'].size(0)

        train_loss = train_loss / n_train

        model.eval()
        test_loss = 0
        n_test = 0
        for batch in valid_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(batch['input_ids'], batch['attention_mask'])
            loss = criterion(outputs, batch['labels'])
            test_loss += loss.item() * batch['input_ids'].size(0)
            n_test += batch['input_ids'].size(0)

        test_loss = test_loss / n_test
        train_losses[it] = train_loss
        test_losses[it] = test_loss

        dt = datetime.now() - t0
        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, '
              f'Test Loss: {test_loss:.4f}, Duration: {dt}')

    return train_losses, test_losses

def evaluate(model, data_loader):
    model.eval()
    n_correct = 0
    n_total = 0

    with torch.no_grad():
        for batch in data_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(batch['input_ids'], batch['attention_mask'])
            _, predictions = torch.max(outputs, 1)

            n_correct += (predictions == batch['labels']).sum().item()
            n_total += batch['labels'].shape[0]

    return n_correct / n_total

# Main execution
if __name__ == '__main__':
    # Set device
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Load and prepare dataset
    checkpoint = 'distilbert-base-cased'
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    raw_datasets = load_dataset('glue', 'sst2')

    def tokenize_fn(batch):
        return tokenizer(batch['sentence'], truncation=True)

    tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)
    tokenized_datasets = tokenized_datasets.remove_columns(['sentence', 'idx'])
    tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # Create data loaders
    train_loader = DataLoader(
        tokenized_datasets['train'],
        shuffle=True,
        batch_size=32,
        collate_fn=data_collator
    )
    valid_loader = DataLoader(
        tokenized_datasets['validation'],
        batch_size=32,
        collate_fn=data_collator
    )

    # Initialize model
    model = Encoder(
        vocab_size=tokenizer.vocab_size,
        max_len=tokenizer.model_max_length,
        d_k=16,
        d_model=64,
        n_heads=4,
        n_layers=2,
        n_classes=2,
        dropout_prob=0.1
    )
    model.to(device)

    # Training setup
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters())

    # Train the model
    train_losses, test_losses = train(
        model, criterion, optimizer, train_loader, valid_loader, epochs=4
    )

    # Evaluate
    train_acc = evaluate(model, train_loader)
    test_acc = evaluate(model, valid_loader)
    print(f'Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')